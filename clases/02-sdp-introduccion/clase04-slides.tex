\documentclass[aspectratio=169,12pt,spanish]{beamer}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}

\usepackage{wrapfig}

%\usepackage{multicol}
%\usepackage{mathtools}

\usepackage[normalem]{ulem}

\usepackage{pgf,tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{arrows}

%\usepackage{wrapfig}
\mode<presentation>
\usefonttheme{professionalfonts}
\usetheme{Darmstadt}
\usecolortheme{orchid}
\useoutertheme{default}
\setbeamertemplate{headline}{}

\newcounter{savedenum}
\newcommand*{\saveenum}{\setcounter{savedenum}{\theenumi}}
\newcommand*{\resume}{\setcounter{enumi}{\thesavedenum}}

\renewcommand{\baselinestretch}{1.1}

%gets rid of bottom navigation bars
\setbeamertemplate{footline}[page number]

%gets rid of navigation symbols
\setbeamertemplate{navigation symbols}{}

%\frameframe{none} % No default frame

%\setlength{\framewidth}{8.7in} \setlength{\frameheight}{7.2in}

\parindent 0pt
\setlength{\parskip} {1ex plus 0.5ex minus 0.2ex}


\usepackage[bbgreekl]{mathbbol}
\usepackage{amssymb, amsthm, amsmath}
\usepackage{bm}

\newtheorem{ejercicio}{Ejercicio}
\newtheorem{proposition}[theorem]{Proposición}

\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
\DeclareSymbolFontAlphabet{\mathbbl}{bbold}

\usepackage{multicol}
\usepackage{colortbl}
\usepackage{lmodern}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}

\graphicspath{ {../../images} }
\input{../../latex/preamble}

\pagestyle{empty}

\begin{document}

%------------------------------------------------------------------

\begin{frame}

 \begin{center}

\Large\textbf{Optimización Semidefinida} \\
\large\textbf{Clase 04 - Matrices definidas positivas}
%\vspace{0.5cm}

% \textit{Santiago Laplagne} \\
%slaplagn@dm.uba.ar \\


%\vspace{0.5cm}
%{\small Trabajo en progreso en conjunto con \emph{Jose Capco} (Universit\"at Innsbruck) y \emph{Claus Scheiderer} %(Universit\"at Konstanz).} \\

\vspace{1cm}
 Segundo Cuatrimestre 2021
 \\
 {\small Facultad de Ciencias Exactas y Naturales, UBA}
 \end{center}

\end{frame}



%------------------------------------------------------------------

\begin{frame}
\frametitle{Motivación}

Consideremos una sucesión de vectores definida recursivamente por
$$
\xb[k + 1] = \Ab \xb[k], \quad \xb[0] = \xb_0
$$

El sistema tiende a 0 para todo vector inicial si y solo todos los autovalores de $\Ab$ tienen módulo menor que 1.

Dada una matriz $\Ab$ con autovalores $\lambda_1, \dots, \lambda_n$, los autovalores de $\Ab - \alpha I$ son $\lambda_1 - \alpha, \dots, \lambda_n - \alpha$.

Si $\Ab$ es simétrica, todos sus autovalores son reales, y podemos calcular su radio espectral calculando el mayor y menor autovalor.

En este caso, podemos plantear el problema de hallar el menor autovalor como un problema de optimización:
$$
\min \{\lambda : \lambda \text{ autovalor de $\Ab$}\} = \sup_{\alpha \in \R} \{\alpha : \Ab - \alpha I \succeq 0\}.
$$

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Matrices simétricas}

Notamos $\Sym^n$ al espacio de matrices simétricas
$$
\Sym^n = \{A \in \R^{n \times n} | a_{ij} = a_{ji}, \forall 1 \le i, j \le n\}.
$$
Es un subespacio vectorial de $\R^{n \times n}$ de dimensión $\frac{n(n+1)}{2}$.

\begin{proposition}
Si $\Ab$ es simétrica, entonces
\begin{enumerate}
\item $\xb^* \Ab \xb \in \R$ para todo $\xb \in \C^n$,
\item \label{item:realav} todos los autovalores de $\Ab$ son reales,
\item autovectores correspondientes a autovalores distintos son perpendiculares,
\item $\Sb^T \Ab \Sb$ es simétrica para toda $\Sb \in \Rnn$.
\end{enumerate}
\end{proposition}


\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Teorema espectral para matrices simétricas}

\begin{theorem}
Cualquier matriz simétrica $\Ab \in \Symn$ admite una descomposición
$$
\Ab = \Ub \Db \Ub^T,
$$
con $\Ub \in \Rnn$ matriz ortogonal y $\Db \in \Rnn$ matriz diagonal, con los autovalores $\{\lambda_1, \dots, \lambda_n\} \subset \R$ de $\Ab$ en la diagonal.
%Equivalentemente,
%$$
%\Ab = \sum_{i=1}^n \lambda_i u_i u_i^T,
%$$
%con $u_i \in \R_n$, $1 \le i \le n$, los autovectores de $\Ab$, que se corresponden con las columnas de $\Pb$.
\end{theorem}

\textbf{Idea de la demostración.} Para cualquier matriz $\Xb \in \Rnn$ podemos calcular la descomposición de Schur
$$
\Xb = \Ub \Tb \Ub^T,
$$
con $\Ub$ matriz ortogonal y $\Tb$ triangular superior. Si $\Xb$ es simétrica, $\Tb$ resulta diagonal.
Ver \cite[Teorema 4.1.5]{Horn1985}.


\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Congruencia de matrices}

En el caso de matrices simétricas existe una diagonalización más simple que la diagonalización por matrices semejantes, que es especialmente útil en problemas de programación semidefinida.

\begin{definition}
Dos matrices $\Ab, \Bb \in \Rnn$ son \emph{congruentes} si existe una matrix $\Sb \in \Rnn$ inversible tal que
$$
\Ab = \Sb \Bb \Sb^T.
$$
\end{definition}

La relación de congruencia es una relación de equivalencia.

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Signatura de una matriz}

Dada una matriz simétrica $\Ab \in \Symn$ definimos la \emph{signatura} de $\Ab$ como la terna
$$(n_+, n_-, n_0),$$
donde
\begin{itemize}
\item $n_+$ la cantidad de autovalores positivos de $\Ab$,
\item $n_-$ la cantidad de autovalores negativos,
\item $n_0$ es la cantidad de autovalores nulos.
\end{itemize}

Si $\Ab \in \Symn$, podemos escribir $\Ab = \Ub \Lambdab \Ub^T$ con $\Lambdab =  \diag(\lambda_1, \dots, \lambda_n)$, la matriz con los autovalores de $\Ab$ en la diagonal, y $\Ub$ unitaria. Suponemos que los autovalores positivos son los primeros, luego los autovalores negativos y finalmente los autovalores cero.


\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Ley de inercia}

Definiendo la matriz diagonal no-singular real
$$\Db = \diag(\sqrt{\lambda_1}, \dots, \sqrt{\lambda_{n_+}}, \sqrt{-\lambda_{n_{+}+1}}, \dots, \sqrt{-\lambda_{n_{+}+n_{-}}}, 1, \dots, 1),$$
obtenemos
$$
\Lambdab = \Db \begin{pmatrix}
1 &        &   &    &        &    &   &        &   \\
  & \ddots &   &    &        &    &   &        &   \\
  &        & 1 &    &        &    &   &        &   \\
  &        &   & -1 &        &    &   &        &   \\
  &        &   &    & \ddots &    &   &        &   \\
  &        &   &    &        & -1 &   &        &   \\
  &        &   &    &        &    & 0 &        &   \\
  &        &   &    &        &    &   & \ddots &   \\
  &        &   &    &        &    &   &        & 0 \\
\end{pmatrix}
\Db.$$

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Ley de inercia}

Podemos escribir a la matriz $\Ab$ como
$$
\Ab = \Ub \Lambdab \Ub^T = \Sb
\begin{pmatrix}
1 &        &   &    &        &    &   &        &   \\
  & \ddots &   &    &        &    &   &        &   \\
  &        & 1 &    &        &    &   &        &   \\
  &        &   & -1 &        &    &   &        &   \\
  &        &   &    & \ddots &    &   &        &   \\
  &        &   &    &        & -1 &   &        &   \\
  &        &   &    &        &    & 0 &        &   \\
  &        &   &    &        &    &   & \ddots &   \\
  &        &   &    &        &    &   &        & 0 \\
\end{pmatrix}
\Sb^T = \Sb \Ib(\Ab) \Sb^T,$$
donde $\Sb = \Ub \Db$ es una matriz no-singular e $\Ib(\Ab)$ es la matriz de inercia de $\Ab$.

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Ley de inercia}

Utilizando esta escritura, la ley de inercia nos permite determinar fácilmente si dos matrices simétricas son congruentes.

\begin{theorem}
Dos matrices simétricas $\Ab, \Bb \in \Symn$ son congruentes si y solo si tienen la misma signatura.
\end{theorem}

\textbf{Demostración.}
Si dos matrices $\Ab, \Bb$ tienen la misma signatura, utilizando la construcción anterior, obtenemos que $\Ab$ y $\Bb$ son congruentes a la misma matriz de inercia. Como la relación de congruencia es una relación de equivalencia, las matrices $\Ab$ y $\Bb$ son congruentes.
\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Ley de inercia (continacuón de la demostración)}

Para la otra implicación, suponemos $\Ab$ y $\Bb$ congruentes, con $\Ab = \Sb \Bb \Sb^T$, para alguna matriz no-singular $\Sb \in \Rnn$. Como las matrices congruentes tienen el mismo rango, $n_0(\Ab) = n_0(\Bb)$ y alcanza ver que $n_+(\Ab) = n_+(\Bb)$. Notamos $k = n_+(\Ab)$.

Sean $\vb_1, \dots, \vb_k$ autovectores ortonormales de $\Ab$ correspondientes a los autovalores positivos $\lambda_1(\Ab), \dots, \lambda_{k}$ y sea $S_+(\Ab) = \langle \vb_1, \dots, \vb_{k} \rangle$.

La dimensión de $S_+(\Ab)$ es $k$, y si
$$\xb = \alpha_1 \vb_1 + \dots + \alpha_k \vb_{k} \neq 0,$$
entonces
$$\xb^T \Ab \xb = \lambda_1(\Ab) |\alpha_1|^2 + \dots + \lambda_{k}(\Ab) |\alpha_k|^2 > 0.$$

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Ley de inercia (continación de la demostración)}

Por lo tanto,
$$
\xb^T (\Sb \Bb \Sb^T) \xb = (\Sb^T\xb)^T \Bb (\Sb^T \xb) > 0,
$$
luego $\yb^T \Bb \yb > 0$ para todo vector no nulo $\yb \in \langle \Sb^T \vb_1, \dots, \Sb^T \vb_k \rangle$, que es un espacio de dimensión $k$. Por lo tanto, $n_+(\Bb) \ge k = n_+(\Ab)$ y realizando el mismo razonamiento intercambiando $\Ab$ por $\Bb$, obtenemos $n_+(\Ab) = n_+(\Bb)$.

\vspace{1cm}

En particular, cualquier matriz simétrica es congruente a una matriz diagonal con valores $0$, $+1$ o $-1$ en la diagonal, y la cantidad de cada uno de estos valores en la diagonal es invariante, no depende de la matriz $\Sb$.

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Diagonalización por congruencia}

Dada una matriz simétrica $\Ab \in \Symn$ podemos obtener una matriz diagonal $\Db$ congruente a $\Ab$ por eliminación gaussiana (realizando simultáneamente eliminación en filas y columnas).

Por lo tanto, podemos calcular eficientemente tanto la diagonalización por congruencia como la signatura de la matriz.

\begin{example}
Calcular una matriz $\Db$ diagonal que sea congruente a
$$
\Ab =
\begin{pmatrix}
3 & 0 & 6 \\
0 & 0 & 1 \\
6 & 1 & 1
\end{pmatrix}.
$$
Calcular los autovalores de $\Ab$ y verificar la ley de inercia.
\end{example}

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Matrices definidas positivas}

Una matriz $\Ab \in \R^{n \times n}$ se llama \emph{semidefinida positiva} si es simétrica y
$$
\xb^T \Ab \xb  \ge 0
$$
para todo $\xb \in \R^n$. Si además $\xb^T \Ab \xb = 0$ solo para $\xb = 0$, decimos que la matriz es \emph{definida positiva}.

Notamos $\Ab \succ 0$ a las matrices definidas positivas y $\Ab \succeq 0$ a las matrices semidefinidas positivas.

Si $\Ab = \Mb^T \Mb$ para $\Mb \in \R^{m \times n}$ cualquiera, entonces
$$
\xb^T \Ab \xb = \xb^T \Mb^T \Mb \xb = \| \Mb \xb \|^2 \ge 0,
$$
lo que nos da un amplio stock de matrices positivas semidefinidas.

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Caracterización de matrices semidefinidas positivas}

Dado un conjunto $S \subset \{1, \dots, n\}$, definimos el menores principales asociado a $S$ como el determinante de la submatriz cuadrada  $A_{S,S}$ formada por las filas y columnas de $A$ con índices en $S$.

\begin{theorem}
  Si $A \in \Rnn$ es un matriz simétrica, las siguientes propiedades son equivalentes:
  \begin{enumerate}
  \item $\Ab$ es semidefinida positiva ($\xb^T \Ab \xb \ge 0$ para todo $\xb \in \R^n$),
  \item todos los autovalores de $\Ab$ son no-negativos,
  \item $\Ab = \Mb^T \Mb$ para alguna matriz $\Mb \in \R^{n \times n}$,
  \item \emph{todos} los menores principales de $\Ab$ son no-negativos.
  \end{enumerate}
\end{theorem}

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Caracterización de matrices definidas positivas}

Para matrices definidas positivas, obtenemos equivalencias similares.

\begin{theorem}
  Si $\Ab \in \Rnn$ es un matriz simétrica, las siguientes propiedades son equivalentes.
  \begin{enumerate}
  \item $\Ab$ es definida positiva ($\xb^T \Ab \xb > 0$ para todo $\xb \in \R^n$),
  \item todos los autovalores de $\Ab$ son positivos,
  \item \label{MTM} $\Ab = \Mb^T \Mb$ para alguna matriz $\Mb \in \R^{n \times n}$ no singular,
  \item \label{menoresprincipales} todos los menores principales de $\Ab$ son positivos,
  \item el polinomio característico de $\Ab$ tiene signos alternados: si $\chi_{\Ab}(t) = t^n + a_{n-1} t^{n-1} + a_1 t + a_0$, entonces $a_i a_{i+1} < 0$ para todo $0 \le i \le {n-1}$ (definiendo $a_n = 1$).
  \end{enumerate}
\end{theorem}

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Menores principales de cabeza}

Si $S = \{1, 2, \dots, k\}$, decimos que el menor asociado es un \emph{menor principal de cabeza}.

En el caso de matrices definidas positivas, podemos restringir la condición (\ref{menoresprincipales}) a considerar solo los menores principales de cabeza.

\textbf{Idea de la demostración.} Usamos que para una matriz simétrica $\Ab$, los autovalores de
$$
\hat \Ab = \begin{pmatrix}
\Ab & \yb \\
\yb^T & a
\end{pmatrix},$$
con $\yb \in \R^n$ y $a \in \R$ están entrecruzados con los autovalores de $\Ab$ (ver \cite[Teorema 4.3.8]{Horn1985}).


\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Factorización de Cholesky}

En ambos casos, la condición (\ref{MTM}) podemos restringirla a matrices triangulares inferiores. Recordemos que cualquier matriz $\Cb \in \Rnn$ admite una descomposición $\Qb\Rb$ y puede escribirse como $\Cb = \Qb \Rb$ con $\Qb$ unitaria y $\Rb$ triangular superior del mismo rango que $\Cb$. Luego
$$
\Ab = \Cb^T \Cb = (\Qb \Rb)^T \Qb \Rb = \Rb^T \Qb^T \Qb \Rb = \Rb^T \Rb
$$
y tomando $\Lb = \Rb^T$, obtenemos $\Ab = \Lb \Lb^T$.

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Factorización de Cholesky}


Si $\Cb$ es no singular, podemos elegir $\Rb$ con todos los valores en la diagonal positivos (y la descomposición de esta forma es única).
Esto prueba el siguiente corolario, que nos da la \emph{descomposición de Cholesky} de una matriz definida positiva.

\begin{corollary}
Una matriz $\Ab$ es definida positiva si y solo si existe una matriz trinagular inferior $\Lb \in \Rnn$ con valores positivos en la diagonal tal que $\Ab = \Lb \Lb^T$.
\end{corollary}



\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Conos}


Un subconjunto $\CC$ de un espacio vectorial real $V$ es un \emph{cono convexo} si es cerrado por combinaciones lineales positivas. Es decir,
\begin{enumerate}
\item Si $\xb \in \CC$ y $a \ge 0$, entonces $a\xb \in \CC$.
\item Si $\xb, \yb \in \CC$, entonces $\xb + \yb \in \CC$.
\end{enumerate}

En este apunte siempre que hablemos de \emph{conos} vamos a referirnos a conos convexos.

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Conos}

Decimos que un cono es \emph{puntiagudo} si
$$
\xb  \in \CC \text{ y } -\xb \in \CC \Rightarrow \xb = 0.
$$

El \emph{cono dual} de un cono $\CC$ es
$$
\CC^* = \{\yb \in \R^n \mid \xb^T \yb \ge 0 \enspace \forall \xb \in \CC\}.
$$

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Orden parcial}

Dado un cono puntiagudo $\CC$ en $\R^n$, podemos definir un orden parcial en $\R^n$ por
$$
\xb \succeq \yb \iff \xb - \yb \in \CC
$$
para $\xb, \yb \in \R^n$. Este orden satisface las siguientes propiedades:
\begin{itemize}
\item \textbf{reflexividad:} $\forall \xb \in \R^n: \xb \succeq \xb$
\item \textbf{antisimetría:} $\forall \xb, \yb \in \R^n: \xb \succeq \yb, \yb \succeq \xb \Rightarrow \xb = \yb$
\item \textbf{transitividad:} $\forall \xb, \yb, \zb \in \R^n: \xb \succeq \yb, \yb \succeq \zb \Rightarrow \xb \succeq \zb$
\item \textbf{homogeneidad:} $\forall \xb, \yb \in \R^n, \forall \alpha \in \R_{\ge 0}: \xb \succeq \yb \Rightarrow \alpha \xb \succeq \alpha \yb$
\item \textbf{aditividad:} $\forall \xb, \yb, \xb', \yb' \in \R^n: \xb \succeq \yb, \xb' \succeq \yb' \Rightarrow \xb + \xb' \succeq \yb + \yb'$
\end{itemize}

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Desigualdad estricta}

En general, nos van a interesar conos convexos de dimensión completa, es decir conos con interior no vacío. En ese caso, podemos definir la desigualdad estricta
$$
\xb \succ \yb \iff \xb - \yb \in \interior \CC.
$$

\end{frame}



%------------------------------------------------------------------

\begin{frame}
\frametitle{Separación}

Tenemos el siguiente resultado de separación.

\begin{lemma} Sean $\CC \subset \R^n$ un cono convexo cerrado y $\xb \in \R^n \smallsetminus \CC$ un punto fuera de $\CC$. Existe un hiperplano que separa a $\{\xb\}$ de $\CC$. Más aún, existe un vector no nulo $\cb \in \R^n$ tal que
$$
\forall \yb \in \R^n : \inner{\cb}{\yb} \ge 0 > \inner{\cb}{\xb}.
$$
\end{lemma}

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Ejemplos}

\noindent\textbf{Cono generado por un conjunto de puntos.}
El cono generado por un conjunto de puntos $A = \{x_1, \dots, x_N\} \subset \R^n$ es el menor cono que contiene a $A$. Es el cono
$$
\cone A = \left\{ \sum_{i=1}^N \alpha_i x_i \mid \alpha_1, \dots, \alpha_N \in \R_{\ge 0}\right\}.
$$

\noindent\textbf{El ortante no-negativo.}
El ortante no-negativo que utilizamos en programación lineal, definido por
$$
\R^n_{\ge 0} = \{(x_1, \dots, x_n) \in \R^n \mid x_1, \dots, x_n \ge 0\}
$$
es un cono puntiagudo convexo cerrado de dimensión completa.

\end{frame}

%------------------------------------------------------------------

\begin{frame}
\frametitle{Ejemplo: el cono de matrices semidefinidas positivas}

\begin{proposition}
El conjunto $\Splusn$ de matrices semidefinidas positivas es un cono convexo.
\end{proposition}

\textbf{Demostración.}
Podemos escribir a $\Splusn$ como intersección de infinitos semiespacios:
$$
\Splusn = \{\Ab \in \Symn \mid \xb^T \Ab \xb \ge 0 \enspace \forall \xb \in \R^n\} = \bigcap_{\xb\in\R^n} \{\Ab \in \Symn \mid \xb^T \Ab \xb \ge 0\},
$$
y por lo tanto es un conjunto convexo cerrado.
Para ver que forman un cono, si $\Ab, \Bb \succeq 0$ y $a \ge 0$, es claro que $a \Ab \succeq 0$ y también que
$$\xb^T (\Ab + \Bb) \xb = \xb^T \Ab \xb + \xb^T \Bb \xb \ge 0$$
 para todo $\xb \in \R^n$.

\end{frame}


%------------------------------------------------------------------

\begin{frame}
\frametitle{Referencias}

\bibliographystyle{apalike}
\bibliography{../../latex/optimizacion}

\end{frame}

\end{document}

%------------------------------------------------------------------

%------------------------------------------------------------------

\begin{frame}
\frametitle{Costo reducido}

La siguiente propiedad será clave para estudiar la dualidad en programación semidefinida.

\begin{theorem}
Una matriz $A$ es semidefinida positiva si y solo si $\innerTrace{A}{X} \ge 0$ para toda matriz $X \in \Splus$.
\end{theorem}

\begin{proof}
Si $A \succeq 0$, entonces $A = B^T B$ para alguna matriz $B$ y por lo tanto,
$$
\innerTrace{A}{X} = \tr(AX) = \tr(B^T B X) = \tr(B X B^T).
$$

Como $X \succeq 0$, también $B X B^T \succeq 0$ y por lo tanto $\tr(B X B^T) \ge 0$.

Para la otra dirección, si $x \in \R^n$, entonces $x x^T \in \Rnn$ es una matriz semidefinida positiva. Si $\langle A, X\rangle \ge 0$ para toda $X$, entonces $\innerTrace{A}{x x^T}  \ge 0$ para todo $x \in \Rn$ y
$$
\innerTrace{A}{xx^T} = \tr(A x x^T) = \tr(x^T A x) = x^T A x
$$
y por lo tanto $x^T A x \ge 0$ para todo $x$.
\end{proof}

